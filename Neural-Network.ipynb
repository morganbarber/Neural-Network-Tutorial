{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f0e0d90",
   "metadata": {},
   "source": [
    "# Creating a Neural Network from Scratch using Numpy\n",
    "\n",
    "In this notebook, we will create a neural network from scratch using only Numpy. We will then use this neural network to perform a binary classification task on a simple dataset. We will begin by explaining the mathematics behind neural networks and then implement the neural network using Python and Numpy.\n",
    "\n",
    "## The Mathematics of Neural Networks\n",
    "\n",
    "A neural network consists of layers of interconnected neurons. Each neuron computes a weighted sum of its inputs, adds a bias term, and then applies an activation function to the result. Mathematically, the output of a neuron can be represented as:\n",
    "\n",
    "$$\n",
    "y = f(w^T x + b)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x$ is the input vector,\n",
    "- $w$ is the weight vector,\n",
    "- $b$ is the bias term, and\n",
    "- $f$ is the activation function.\n",
    "\n",
    "For a binary classification task, we can use the sigmoid activation function, defined as:\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "In this example, we will create a simple feedforward neural network with one input layer, one hidden layer, and one output layer. The input layer will have two neurons (for two input features), the hidden layer will have three neurons, and the output layer will have one neuron (for binary classification).\n",
    "\n",
    "## Initializing Weights and Biases\n",
    "\n",
    "We will initialize the weights and biases randomly using a normal distribution with mean 0 and standard deviation 1. We will use the following function to initialize the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f75006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(hidden_size, input_size)\n",
    "    b1 = np.random.randn(hidden_size, 1)\n",
    "    W2 = np.random.randn(output_size, hidden_size)\n",
    "    b2 = np.random.randn(output_size, 1)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b1bca",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Next, we will implement the forward propagation step, where we compute the outputs of each layer given the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1b918",
   "metadata": {},
   "source": [
    "## Backward Propagation\n",
    "\n",
    "After the forward propagation step, we need to compute the gradients of the weights and biases with respect to the loss function. We will use the binary cross-entropy loss function, defined as:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "To compute the gradients, we will use the chain rule, which states that the gradient of a composed function is the product of the gradients of its constituent functions:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "We will implement the backward propagation step as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, Z1, A1, Z2, A2, W1, b1, W2, b2):\n",
    "    m = X.shape[1]\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * sigmoid_derivative(Z1)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d4f70",
   "metadata": {},
   "source": [
    "## Updating Weights and Biases\n",
    "\n",
    "Now that we have the gradients, we can update the weights and biases using gradient descent:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "We will implement this step as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53afaf",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "Finally, we can train the neural network using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7461d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, Y, epochs, learning_rate):\n",
    "    input_size = X.shape[0]\n",
    "    hidden_size = 3\n",
    "    output_size = 1\n",
    "\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, Y, Z1, A1, Z2, A2, W1, b1, W2, b2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80571d",
   "metadata": {},
   "source": [
    "We can now train the neural network on our dataset and use the trained weights and biases to make predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
